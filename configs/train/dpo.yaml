train_method_name: "dpo"
finetuning_frequency: 400

# Config
dpo_strategy: 2
dpo_config:
  beta: 0.4
  max_seq_length: 6500
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  gradient_checkpointing: 1
  num_train_epochs: 2
  learning_rate: 1e-5
  lr_scheduler_type: 'cosine'
  weight_decay: 0.001
  warmup_steps: 0
  logging_steps: 1
  f_divergence_type: "alpha_divergence" # "alpha_divergence" amounts to forward KL, other options are "js_divergence", "reverse_kl" (default)
  f_alpha_divergence_coef: 1.0 # won't be used for js_divergence or reverse_kl
  #optim: "adamw_8bit"
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5384d3-8451-47dc-9e27-f60e32626d4d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ff52c5-97ce-48b4-9feb-80d929764128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_17:22:02_PDT_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.103\n",
      "Build cuda_12.3.r12.3/compiler.33492891_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554e8686-cd4a-44b9-bb98-9b117af01b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/nevali/miniforge3/envs/funrlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e546782-ab36-48a0-815c-1eb0da9b2276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "GPU name: None\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db63e8-644b-41d0-a604-d1f9305aad86",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c5c54-e499-4e18-a867-a1fe967caf07",
   "metadata": {},
   "source": [
    "### Load the data and check the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9c414f-d700-43a2-96d3-e77385220cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Dataset Features:\n",
      "                                   uuid  \\\n",
      "0  6d978758-3406-5b33-883a-2ddcd25e3d7c   \n",
      "1  cb47e985-3d7c-57f7-9c6f-00231e6784b3   \n",
      "2  7b4cecf5-9b18-5fbb-be21-e7b7dc29d7ae   \n",
      "3  6313871b-19ae-5199-8f6e-bf65cbe4d2e5   \n",
      "4  767cd4cb-0e7a-57da-a46d-939b8e8c34a1   \n",
      "\n",
      "                                         instruction  \\\n",
      "0  I am trying to determine if a given computer p...   \n",
      "1  I have a question about generative models, spe...   \n",
      "2  I have a cooled magnet and a topological insul...   \n",
      "3  I'm looking for a way to create a RESTful API ...   \n",
      "4  What are the significance and organizational s...   \n",
      "\n",
      "                      gen_input_configs  \\\n",
      "0   {'temperature': 1.0, 'top_p': 0.99}   \n",
      "1  {'temperature': 1.0, 'top_p': 0.995}   \n",
      "2  {'temperature': 1.1, 'top_p': 0.995}   \n",
      "3   {'temperature': 1.0, 'top_p': 0.99}   \n",
      "4  {'temperature': 1.25, 'top_p': 0.99}   \n",
      "\n",
      "                                              intent  \\\n",
      "0  The user wants to evaluate the performance of ...   \n",
      "1  The user wants to know common pitfalls when tr...   \n",
      "2  The user wants to detect the propagation of Ma...   \n",
      "3  The user wants to create a RESTful API to inte...   \n",
      "4  The user wants to understand the concept of AI...   \n",
      "\n",
      "                                           knowledge difficulty input_quality  \\\n",
      "0  To solve this problem, the models need to know...     medium          good   \n",
      "1  To solve this problem, the models need to know...     medium          good   \n",
      "2  To solve this problem, the models need to know...       hard          good   \n",
      "3  To solve this problem, the models need to know...     medium          good   \n",
      "4  To solve this problem, the models need to know...     medium          good   \n",
      "\n",
      "                                 quality_explanation        task_category  \\\n",
      "0  The query is clear and specific about the user...  Information seeking   \n",
      "1  The user query is clear and specific, asking a...  Information seeking   \n",
      "2  The query is clear and specific, with a clear ...  Information seeking   \n",
      "3  The query is clear and concise, stating the sp...   Coding & Debugging   \n",
      "4  The query is clear and specific, asking about ...  Information seeking   \n",
      "\n",
      "   input_length                                          responses  \\\n",
      "0           152  [Evaluating the performance of a computer prog...   \n",
      "1           158  [Generative Adversarial Networks (GANs) can be...   \n",
      "2           108  [What a fascinating topic!\\n\\nTo detect the pr...   \n",
      "3           105  [Here's a step-by-step guide to creating a RES...   \n",
      "4           100  [The concept of AI governance refers to the de...   \n",
      "\n",
      "                                gen_response_configs  \\\n",
      "0  {'engine': 'vllm', 'max_tokens': 4096, 'output...   \n",
      "1  {'engine': 'vllm', 'max_tokens': 4096, 'output...   \n",
      "2  {'engine': 'vllm', 'max_tokens': 4096, 'output...   \n",
      "3  {'engine': 'vllm', 'max_tokens': 4096, 'output...   \n",
      "4  {'engine': 'vllm', 'max_tokens': 4096, 'output...   \n",
      "\n",
      "                                      rewards_armorm  \\\n",
      "0  [{'score': 0.1845703125}, {'score': 0.18554687...   \n",
      "1  [{'score': 0.173828125}, {'score': 0.1796875},...   \n",
      "2  [{'score': 0.19140625}, {'score': 0.18359375},...   \n",
      "3  [{'score': 0.109375}, {'score': 0.103515625}, ...   \n",
      "4  [{'score': 0.2001953125}, {'score': 0.20507812...   \n",
      "\n",
      "                                              chosen  \\\n",
      "0  [{'content': 'I am trying to determine if a gi...   \n",
      "1  [{'content': 'I have a question about generati...   \n",
      "2  [{'content': 'I have a cooled magnet and a top...   \n",
      "3  [{'content': 'I'm looking for a way to create ...   \n",
      "4  [{'content': 'What are the significance and or...   \n",
      "\n",
      "                                            rejected  \n",
      "0  [{'content': 'I am trying to determine if a gi...  \n",
      "1  [{'content': 'I have a question about generati...  \n",
      "2  [{'content': 'I have a cooled magnet and a top...  \n",
      "3  [{'content': 'I'm looking for a way to create ...  \n",
      "4  [{'content': 'What are the significance and or...  \n",
      "\n",
      "ðŸ“¦ Available columns: ['uuid', 'instruction', 'gen_input_configs', 'intent', 'knowledge', 'difficulty', 'input_quality', 'quality_explanation', 'task_category', 'input_length', 'responses', 'gen_response_configs', 'rewards_armorm', 'chosen', 'rejected']\n"
     ]
    }
   ],
   "source": [
    "# --- Load Dataset ---\n",
    "dataset_id = \"Magpie-Align/Magpie-Air-DPO-100K-v0.1\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "# Convert to Pandas DataFrame for easy viewing\n",
    "df = pd.DataFrame(dataset)\n",
    "print(\"ðŸ§¾ Dataset Features:\")\n",
    "print(df.head())\n",
    "print(\"\\nðŸ“¦ Available columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba07ff1-2b3a-4511-91fa-d10fafd8aac3",
   "metadata": {},
   "source": [
    "### Check one sample to see what we are talking about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523e0ac8-60be-4117-bfdc-8baeaefd84a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am trying to determine if a given computer program is a good solution for a specific problem. How do I evaluate the performance of a computer program?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed8779c-afeb-407d-a958-4076b0405fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 1.0, 'top_p': 0.99}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"gen_input_configs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c7552a-6a86-42d8-9664-315d99382db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The user wants to evaluate the performance of a computer program'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963d76ce-90cf-4997-bf99-71ab18777811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To solve this problem, the models need to know about program evaluation, performance metrics, and possibly programming concepts'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"knowledge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01affec1-98d1-4984-80db-08f4a9c9783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1dd1fd5-bd6d-41b4-aacd-59626fa68b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"input_quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d35e5837-eb9c-4982-a14f-e07dff18b172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The query is clear and specific about the user's goal, which is to evaluate the performance of a computer program. The user provides a specific context, which is helpful. However, the query could be improved by providing more details about the specific problem they are trying to solve and what they mean by 'good solution'. Additionally, the query could benefit from more specificity about what aspects of the program's performance they want to evaluate.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"quality_explanation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c72fd66a-d679-44da-8681-c74087c508a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Information seeking'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"task_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4f10639-53fe-4875-a6cb-6d83f35ca8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(152)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"input_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1473b982-e54b-4a8d-83bf-7ece139c265c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Evaluating the performance of a computer program involves assessing its ability to meet the requirements and needs of the problem it is intended to solve. Here are some key aspects to consider when evaluating the performance of a computer program:\\n\\n1. **Accuracy**: Does the program produce the correct output for a given input? Does it maintain a high level of precision and accuracy?\\n2. **Speed**: How fast does the program execute? Does it respond quickly enough to meet the user's expectations?\\n3. **Efficiency**: Does the program use resources (e.g., memory, CPU, network bandwidth) efficiently? Does it minimize waste and optimize system resources?\\n4. **Scalability**: Can the program handle large or complex inputs? Does it scale well with increasing data sizes or user demand?\\n5. **Stability**: Does the program crash or exhibit unusual behavior under normal use? Is it robust and reliable?\\n6. **Usability**: Is the program easy to use and understand? Are the user interface and user experience intuitive and user-friendly?\\n7. **Maintainability**: Is the program easy to modify, update, and maintain? Are the code and architecture modular and flexible?\\n8. **Security**: Does the program protect user data and prevent unauthorized access or manipulation?\\n9. **Error handling**: Does the program handle errors and exceptions gracefully, providing helpful error messages and debugging information?\\n10. **Code quality**: Is the code readable, maintainable, and consistent? Are there any code smells or anti-patterns?\\n\\nTo evaluate these aspects, you can use various methods, including:\\n\\n1. **Unit testing**: Write automated tests to verify the program's functionality and accuracy.\\n2. **Benchmarking**: Measure the program's performance using benchmarks, such as execution time, memory usage, or throughput.\\n3. **User testing**: Observe users interact with the program and gather feedback on usability and user experience.\\n4. **Code reviews**: Review the codebase for maintainability, readability, and security.\\n5. **System testing**: Test the program under various scenarios, such as different input data, network conditions, or system configurations.\\n6. **Monitoring**: Monitor the program's performance and resource usage over time to identify potential issues or bottlenecks.\\n7. **Comparative analysis**: Compare the program to similar solutions or benchmarks to determine how it performs in relation to others.\\n\\nTo assess the program's performance, you can use various metrics, such as:\\n\\n1. **Throughput**: The number of tasks or operations completed per unit of time.\\n2. **Response time**: The time it takes for the program to respond to user input or requests.\\n3. **Latency**: The delay between the time a task is initiated and the time it is completed.\\n4. **Error rate**: The number of errors or exceptions occurring during program execution.\\n5. **Resource utilization**: The amount of system resources (e.g., CPU, memory, disk space) used by the program.\\n\\nBy evaluating these aspects and metrics, you can gain a comprehensive understanding of the program's performance and identify areas for improvement.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"responses\"][0] # we have 5 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cbe939b-4b63-4178-ab96-4e6d5cb22cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'engine': 'vllm',\n",
       " 'max_tokens': 4096,\n",
       " 'output_generator': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       " 'prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nI am trying to determine if a given computer program is a good solution for a specific problem. How do I evaluate the performance of a computer program?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " 'repetition_penalty': 1.0,\n",
       " 'stop_tokens': ['<|eot_id|>',\n",
       "  '<|end_of_text|>',\n",
       "  '<|starter_header_id|>',\n",
       "  '<|end_header_id|>'],\n",
       " 'temperature': 0.8,\n",
       " 'top_p': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"gen_response_configs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "842c9909-5fa0-4956-ae5a-03f2df9e8a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1845703125},\n",
       " {'score': 0.185546875},\n",
       " {'score': 0.18359375},\n",
       " {'score': 0.189453125},\n",
       " {'score': 0.1826171875}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"rewards_armorm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac350b09-9223-4c4a-a520-c0b57c737a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'I am trying to determine if a given computer program is a good solution for a specific problem. How do I evaluate the performance of a computer program?',\n",
       "  'role': 'user'},\n",
       " {'content': \"Evaluating the performance of a computer program is a crucial step in determining whether it effectively solves a specific problem. Here are some key metrics and methods to help you assess the performance of a computer program:\\n\\n1. **Accuracy**: How well does the program achieve the intended result? Measure the accuracy of the program by comparing its output with the expected output or a known reference solution.\\n2. **Efficiency**: How quickly does the program execute? Measure the program's execution time, memory usage, and CPU usage to evaluate its efficiency.\\n3. **Scalability**: How well does the program handle large inputs or increasing amounts of data? Test the program with larger inputs to ensure it can scale horizontally or vertically.\\n4. **Reliability**: How frequently does the program fail or produce incorrect results? Monitor the program's reliability over time, and test it with various inputs to identify any bugs or issues.\\n5. ** Usability**: How easy is the program to use? Evaluate the program's user interface, documentation, and overall user experience.\\n6. **Maintainability**: How easy is it to modify, update, or fix the program? Assess the program's code quality, commenting, and documentation to determine its maintainability.\\n7. **Robustness**: How well does the program handle unexpected inputs or errors? Test the program with invalid or edge cases to evaluate its robustness.\\n8. **Speed**: How fast does the program complete its tasks? Measure the program's execution time and compare it to other similar programs.\\n9. **Memory usage**: How much memory does the program use? Monitor the program's memory usage, especially for large inputs or long-running tasks.\\n10. **Code quality**: How well-written and organized is the code? Evaluate the program's code structure, naming conventions, and commenting to assess its quality.\\n\\nTo conduct a thorough evaluation, you can use various techniques, such as:\\n\\n1. **Unit testing**: Write test cases to verify the program's functionality for each individual component or function.\\n2. **Integration testing**: Test the program's interactions with other components or systems.\\n3. **System testing**: Test the program as a whole, simulating real-world scenarios and user interactions.\\n4. **Benchmarking**: Compare the program's performance with other similar programs or benchmarks.\\n5. **Code review**: Review the program's source code to identify potential issues, improve code quality, and ensure maintainability.\\n6. **Log analysis**: Analyze program logs to monitor performance, identify issues, and optimize the program.\\n7. **User feedback**: Collect feedback from users to identify areas for improvement and ensure the program meets their needs.\\n\\nWhen evaluating a program, consider the following factors:\\n\\n1. **Problem domain**: Is the program designed to solve a specific problem or set of problems?\\n2. **Requirements**: Does the program meet the specified requirements?\\n3. **User needs**: Does the program meet the users' needs and expectations?\\n4. **Industry standards**: Does the program comply with industry standards and best practices?\\n5. **Future-proofing**: Is the program designed with future updates, changes, and scalability in mind?\\n\\nBy using these metrics and techniques, you can comprehensively evaluate the performance of a computer program and determine whether it is a good solution for a specific problem.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5fae8b7-b5f9-4085-a065-3116dbd15e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Evaluating the performance of a computer program involves assessing its behavior, efficiency, and effectiveness in solving a specific problem or set of tasks. Here are some key factors to consider when evaluating a program's performance:\\n\\n1. **Correctness**: Does the program produce the correct results for a given input? Does it meet the requirements and specifications?\\n2. **Efficiency**: How well does the program use system resources such as CPU, memory, and I/O operations? Is it optimized for the specific problem it's solving?\\n3. **Scalability**: How well does the program perform as the input size or complexity increases? Can it handle large datasets or heavy loads?\\n4. **Reliability**: How often does the program crash, produce errors, or require restarts? Is it stable and reliable in production environments?\\n5. **Usability**: How easy is it to use the program? Is the user interface intuitive, and are the instructions clear?\\n6. **Maintainability**: How easy is it to modify, update, or debug the program? Are the code and documentation well-organized and easy to understand?\\n7. **Performance metrics**: Quantify the program's performance using metrics such as:\\n\\t* Execution time (e.g., seconds, milliseconds)\\n\\t* Memory usage (e.g., bytes, kilobytes)\\n\\t* Throughput (e.g., number of operations per second)\\n\\t* Error rates (e.g., percentage of incorrect results)\\n8. **Comparative analysis**: Compare the program's performance to other similar programs or established benchmarks.\\n9. **Testing and validation**: Thoroughly test the program with a variety of inputs, edge cases, and error scenarios to ensure it works correctly and consistently.\\n10. **Monitoring and logging**: Does the program provide sufficient logging and monitoring capabilities to help diagnose and troubleshoot issues?\\n\\nTo evaluate a program's performance, you can use various testing techniques, such as:\\n\\n1. **Unit testing**: Test individual components or functions to ensure they work correctly.\\n2. **Integration testing**: Test how components interact with each other.\\n3. **System testing**: Test the entire program in a simulated production environment.\\n4. **Stress testing**: Push the program to its limits to identify performance bottlenecks.\\n5. **Load testing**: Test the program with a large amount of data or traffic to evaluate scalability.\\n6. **Regression testing**: Test the program again after changes or updates to ensure they didn't introduce new issues.\\n\\nAdditionally, consider the following tools and techniques to help you evaluate a program's performance:\\n\\n1. **Profiling tools**: Analyze the program's execution time, memory usage, and CPU usage.\\n2. **Benchmarking tools**: Compare the program's performance to industry standards or other programs.\\n3. **Error tracking and logging**: Use tools to track errors, warnings, and exceptions.\\n4. **Code reviews**: Review the code with other developers or experts to identify areas for improvement.\\n\\nBy considering these factors and using various testing and evaluation techniques, you can gain a comprehensive understanding of a computer program's performance and its ability to solve a specific problem.\",\n",
       " 'role': 'assistant'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"rejected\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3f1aa6-8a12-41f9-b416-2eb59aee77d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6562, 0.6602, 0.8008, 0.3887, 0.3730, 0.5859, 0.5742, 0.7031, 0.6445,\n",
       "         0.4551, 0.9180, 0.5391, 0.5352, 0.5469, 0.4082, 0.3867, 0.3828, 0.4258,\n",
       "         0.4434]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_obj_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710b1125-9a63-460c-92cc-6a6887b6b78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.8054e-03, 4.8021e-09, 5.1688e-08, 1.2016e-04, 6.9141e-01, 9.2387e-06,\n",
       "         7.7637e-02, 2.0082e-09, 3.0640e-02, 2.2701e-09, 8.6054e-07, 9.1642e-07,\n",
       "         9.0599e-05, 5.4836e-05, 1.9434e-01, 7.0781e-08, 7.2122e-06, 3.9291e-04,\n",
       "         6.9849e-09]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b6a073-4783-430f-884c-a531202d2554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.8054e-03, 4.8021e-09, 5.1688e-08, 1.2016e-04, 6.9141e-01, 9.2387e-06,\n",
       "         7.7637e-02, 2.0082e-09, 3.0640e-02, 2.2701e-09, 8.6054e-07, 9.1642e-07,\n",
       "         9.0599e-05, 5.4836e-05, 1.9434e-01, 7.0781e-08, 7.2122e-06, 3.9291e-04,\n",
       "         6.9849e-09]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdec8bc9-46d8-491e-9c4a-affff4881e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.8054e-03,  4.8021e-09,  5.1688e-08,  1.2016e-04, -1.0699e-01,\n",
       "          9.2387e-06,  7.7637e-02,  2.0082e-09,  3.0640e-02,  2.2701e-09,\n",
       "          8.6054e-07,  9.1642e-07,  9.0599e-05,  5.4836e-05,  1.9434e-01,\n",
       "          7.0781e-08,  7.2122e-06,  3.9291e-04,  6.9849e-09]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_obj_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99db75a3-aa98-4fc9-ba93-2dacf310dde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1085)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(multi_obj_coeffs.view(-1), multi_obj_rewards.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f778854-d8ec-4707-bb49-318a637a1acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1084])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8df31c2-5644-49a0-83f9-661c43655d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-complexity: 0.19434\n",
      "helpsteer-verbosity: -0.10699\n",
      "ultrafeedback-instruction_following: 0.07764\n",
      "tensor([2.7812, 2.8008, 3.5039, 1.4434, 1.3652])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "device = \"cpu\"\n",
    "path = \"RLHFlow/ArmoRM-Llama3-8B-v0.1\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path, device_map=device, \n",
    "                               trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True)\n",
    "# We load a random sample from the validation set of the HelpSteer dataset\n",
    "prompt = 'What are some synonyms for the word \"beautiful\"?'\n",
    "response = \"Nicely, Beautifully, Handsome, Stunning, Wonderful, Gorgeous, Pretty, Stunning, Elegant\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt},\n",
    "           {\"role\": \"assistant\", \"content\": response}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "   output = model(input_ids)\n",
    "   # Multi-objective rewards for the response\n",
    "   multi_obj_rewards = output.rewards.cpu().float() \n",
    "   # The gating layer's output is conditioned on the prompt\n",
    "   c = output.gating_output.cpu().float()\n",
    "   # The preference score for the response, aggregated from the \n",
    "   # multi-objective rewards with the gating layer\n",
    "   preference_score = output.score.cpu().float()  \n",
    "# We apply a transformation matrix to the multi-objective rewards\n",
    "# before multiplying with the gating layer's output. This mainly aims\n",
    "# at reducing the verbosity bias of the original reward objectives\n",
    "obj_transform = model.reward_transform_matrix.data.cpu().float()\n",
    "# The final coefficients assigned to each reward objective\n",
    "# NOTE: this seems different compared to what they actually do in the paper.\n",
    "multi_obj_coeffs = gating_output @ obj_transform.T\n",
    "# The preference score is the linear combination of the multi-objective rewards with\n",
    "# the multi-objective coefficients, which can be verified by the following assertion\n",
    "assert torch.isclose(torch.sum(multi_obj_rewards * multi_obj_coeffs, dim=1), preference_score, atol=1e-3) \n",
    "# Find the top-K reward objectives with coefficients of the highest magnitude\n",
    "K = 3\n",
    "top_obj_dims = torch.argsort(torch.abs(multi_obj_coeffs), dim=1, descending=True,)[:, :K]\n",
    "top_obj_coeffs = torch.gather(multi_obj_coeffs, dim=1, index=top_obj_dims)\n",
    "\n",
    "# The attributes of the 19 reward objectives\n",
    "attributes = ['helpsteer-helpfulness','helpsteer-correctness','helpsteer-coherence',\n",
    "   'helpsteer-complexity','helpsteer-verbosity','ultrafeedback-overall_score',\n",
    "   'ultrafeedback-instruction_following', 'ultrafeedback-truthfulness',\n",
    "   'ultrafeedback-honesty','ultrafeedback-helpfulness','beavertails-is_safe',\n",
    "   'prometheus-score','argilla-overall_quality','argilla-judge_lm','code-complexity',\n",
    "   'code-style','code-explanation','code-instruction-following','code-readability']\n",
    "\n",
    "example_index = 0\n",
    "for i in range(K):\n",
    "   attribute = attributes[top_obj_dims[example_index, i].item()]\n",
    "   coeff = top_obj_coeffs[example_index, i].item()\n",
    "   print(f\"{attribute}: {round(coeff,5)}\")\n",
    "# code-complexity: 0.19922\n",
    "# helpsteer-verbosity: -0.10864\n",
    "# ultrafeedback-instruction_following: 0.07861\n",
    "\n",
    "# The actual rewards of this example from the HelpSteer dataset\n",
    "# are [3,3,4,2,2] for the five helpsteer objectives: \n",
    "# helpfulness, correctness, coherence, complexity, verbosity\n",
    "# We can linearly transform our predicted rewards to the \n",
    "# original reward space to compare with the ground truth\n",
    "helpsteer_rewards_pred = multi_obj_rewards[0, :5] * 5 - 0.5\n",
    "print(helpsteer_rewards_pred)\n",
    "# [2.78125   2.859375  3.484375  1.3847656 1.296875 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bb324-079a-4735-b846-1ed7292307c5",
   "metadata": {},
   "source": [
    "### Let's try the model on the Magpie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c686691-b85b-4140-8290-d7203ef7bd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

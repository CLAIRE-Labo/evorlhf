from typing import List, Dict

from typing import List, Dict, Tuple
from collections import defaultdict
import torch
from itertools import cycle, product
import copy
import numpy as np
import logging
import wandb
from datasets import DatasetDict, Dataset

# DPO DATASET STRATEGY GENERATION:
# 1. Combine (max passed, min passed) and (max passed, failed) scores
# 2. Combine (bottom half of passed, top half of passed) and (passed, failed)
# 2. (not implemented) Combine (1 2) and (2 3) and failed with min passed
# 3. (not implemented) Combine (1,2) and (3,4) and failed with min passed 
# 0. (not implemented) Combine (max passed, min passed) and (passed_not_used_yet , failed) scores


class DPODataBuffer:
    def __init__(self):
        self.train_dataset = {"prompt": [], "chosen": [], "rejected": [], "chosen_chat_score": []}
        self.scores_since_finetune = []

    def construct_and_add_pairs(self, cfg, passed_classes_previous, failed_classes_previous):
        failed_chats = []
        passed_chats = []
        passed_scores = []
        logging.info(f'DEBUGGGGGGG passed_classes_previous: {passed_classes_previous}')
        logging.info(f'DEBUGGGGGGG failed_classes_previous: {failed_classes_previous}')
        #assert len(passed_classes_previous) + len(failed_classes_previous) == cfg.num_outputs_per_prompt

        # Collect failed chats
        for failed_class in failed_classes_previous:
            assert failed_class.fail_flag == 1
            if failed_class.chat not in failed_chats:
                failed_chats.append(failed_class.chat)

        # Collect chosen chats and their scores
        for passed_class in passed_classes_previous:
            assert passed_class.fail_flag == 0
            if passed_class.chat not in passed_chats:
                passed_chats.append(passed_class.chat)
                passed_scores.append(passed_class.score)
                self.scores_since_finetune.append(passed_class.score)
        logging.info(f'DEBUGGGGGGG passed_chats: {passed_chats}')
        logging.info(f'DEBUGGGGGGG passed_scores: {passed_scores}')
        logging.info(f'DEBUGGGGGGG failed_chats: {failed_chats}')
        assert len(passed_chats) == len(passed_scores)

        pairs = []

        wandb.log(
            {"dpodata/passed_chats": len(passed_chats), "dpodata/failed_chats": len(failed_chats)}
        )

        if cfg.train.dpo_strategy == 1:
            assert ValueError("DPO strategy 1 is not advised to use")
            if passed_chats:
                unique_scores = set(passed_scores)
                wandb.log({"dpodata/unique_passed_scores": len(unique_scores)})

                # Need to prepare here to pair up with failed chats
                highest_score = max(unique_scores)
                highest_scoring_chats = [
                    chat
                    for chat, score in zip(passed_chats, passed_scores)
                    if score == highest_score
                ]
                if len(unique_scores) >= 2:
                    lowest_score = min(unique_scores)
                    assert highest_score != lowest_score
                    lowest_scoring_chats = [
                        chat
                        for chat, score in zip(passed_chats, passed_scores)
                        if score == lowest_score
                    ]

                    for chosen_chat, rejected_chat in zip(
                        highest_scoring_chats, cycle(lowest_scoring_chats)
                    ):
                        pairs.append((chosen_chat, rejected_chat, highest_score))

            # Pair failed chats with highest-scoring chosen chats
            if failed_chats and passed_chats:
                for chosen_chat, rejected_chat in zip(highest_scoring_chats, cycle(failed_chats)):
                    pairs.append((chosen_chat, rejected_chat, highest_score))

        elif cfg.train.dpo_strategy == 2:
            if passed_chats:
                unique_scores = set(passed_scores)
                wandb.log({"dpodata/unique_passed_scores": len(unique_scores)})
                chat_ranking = sorted(
                    zip(passed_chats, passed_scores), key=lambda x: x[1], reverse=True
                )
                if len(unique_scores) >= 2:
                    # Sort the passed_chats and passed_scores together by scores in descending order
                    # Find the highest and lowest scores
                    highest_score = chat_ranking[0][1]
                    lowest_score = chat_ranking[-1][1]
                    assert highest_score != lowest_score
                    # Split the chats into better half and worst half
                    unique_scores = sorted(set(passed_scores), reverse=True)
                    mid_index = len(unique_scores) // 2
                    better_scores = unique_scores[:mid_index]
                    worse_scores = unique_scores[mid_index:]

                    # Collect chats with better scores
                    better_half = [
                        (chat, score) for chat, score in chat_ranking if score in better_scores
                    ]
                    # Collect chats with worse scores
                    worse_half = [
                        (chat, score) for chat, score in chat_ranking if score in worse_scores
                    ]
                    # Pair up better scoring passed chats with worse scoring passed chats
                    for (chosen_chat, chosen_score), (rejected_chat, rejected_score) in zip(
                        better_half, worse_half
                    ):
                        pairs.append((chosen_chat, rejected_chat, chosen_score))

            if failed_chats and passed_chats:
                # passed_chats_with_scores = list(zip(passed_chats, passed_scores))
                for (chosen_chat, chosen_score), rejected_chat in zip(
                    cycle(chat_ranking), failed_chats
                ):
                    pairs.append((chosen_chat, rejected_chat, chosen_score))

        for pair in pairs:
            assert len(pair) == 3
            assert type(pair[0]) == list and type(pair[1]) == list
            assert type(pair[0][0]) == dict and type(pair[1][0]) == dict
            assert type(pair[2]) in [int, float, np.int64, np.float64, np.float32]
            # Add pairs to the dataset
            self.add_chat(pair[0], pair[1], pair[2])

        if cfg.wandb:
            wandb.log({"dpodata/num_of_pairs": len(pairs)})

    def add_chat(self, chosen_conv, rejected_conv, chosen_chat_score):
        assert type(chosen_conv) == list and type(rejected_conv) == list
        assert (
            len(chosen_conv) == len(rejected_conv) == 2
            or len(chosen_conv) == len(rejected_conv) == 3
        )
        assert type(chosen_conv[0]) == dict and type(rejected_conv[0]) == dict
        if len(chosen_conv) == 3:
            assert chosen_conv[0]["role"] == "system" and rejected_conv[0]["role"] == "system"
            assert chosen_conv[1]["role"] == "user" and rejected_conv[1]["role"] == "user"
            assert chosen_conv[2]["role"] == "assistant" and rejected_conv[2]["role"] == "assistant"
        else:
            assert chosen_conv[0]["role"] == "user" and rejected_conv[0]["role"] == "user"
            assert chosen_conv[1]["role"] == "assistant" and rejected_conv[1]["role"] == "assistant"
        assert chosen_conv[0]["content"] == rejected_conv[0]["content"]
        assert chosen_conv[1]["content"] == rejected_conv[1]["content"]
        assert chosen_conv[2]["content"] != rejected_conv[2]["content"]
        assert type(chosen_conv[0]["content"]) == str and type(rejected_conv[0]["content"]) == str
        assert type(chosen_conv[1]["content"]) == str and type(rejected_conv[1]["content"]) == str
        assert type(chosen_conv[2]["content"]) == str and type(rejected_conv[2]["content"]) == str
        prompt = chosen_conv[:-1]
        assert prompt == rejected_conv[:-1], "Prompt mismatch between chosen and rejected convs"

        chosen_response = [chosen_conv[-1]]
        rejected_response = [rejected_conv[-1]]

        # Validate that all components are present
        if prompt and chosen_response and rejected_response:
            self.train_dataset["prompt"].append(prompt)
            self.train_dataset["chosen"].append(chosen_response)
            self.train_dataset["rejected"].append(rejected_response)
            self.train_dataset["chosen_chat_score"].append(chosen_chat_score)
        else:
            assert False, "Incomplete DPO datapoint"

    def add_batch_data(self, cfg, passed_function_classes, failed_function_classes):
        all_function_classes = copy.deepcopy(passed_function_classes + failed_function_classes)

        def group_functions_by_prompt(functions):
            # Dictionary to store functions grouped by prompt_num
            prompt_groups = {}

            # Group functions by prompt_num
            for func_class in functions:
                prompt_num = func_class.prompt_num
                if prompt_num not in prompt_groups:
                    prompt_groups[prompt_num] = []
                prompt_groups[prompt_num].append(func_class)

            # Convert groups to sorted list
            grouped_functions = []
            for prompt_num in sorted(prompt_groups.keys()):
                grouped_functions.append(prompt_groups[prompt_num])

            return grouped_functions

        grouped_functions = group_functions_by_prompt(all_function_classes)
        # assert len(grouped_functions) == cfg.num_cont_rounds
        # Removed the assert because if no function from a single prompt is passed, it will not be included in the grouped_functions

        # Sort the functions that have the same prompt_num together in a separate list
        for functions_with_same_prompt in grouped_functions:
            passed_function_classes_same_prompt = []
            failed_function_classes_same_prompt = []
            for function_class in functions_with_same_prompt:
                if function_class.fail_flag == 0:
                    passed_function_classes_same_prompt.append(function_class)
                else:
                    failed_function_classes_same_prompt.append(function_class)
            self.construct_and_add_pairs(
                cfg, passed_function_classes_same_prompt, failed_function_classes_same_prompt
            )

    def get_dataset_above_threshold(self, threshold):
        filtered_dataset = {"prompt": [], "chosen": [], "rejected": [], "chosen_chat_score": []}
        for prompt, chosen, rejected, score in zip(
            self.train_dataset["prompt"],
            self.train_dataset["chosen"],
            self.train_dataset["rejected"],
            self.train_dataset["chosen_chat_score"],
        ):
            if score >= threshold:
                filtered_dataset["prompt"].append(prompt)
                filtered_dataset["chosen"].append(chosen)
                filtered_dataset["rejected"].append(rejected)
                filtered_dataset["chosen_chat_score"].append(score)
        assert (
            len(filtered_dataset["prompt"])
            == len(filtered_dataset["chosen"])
            == len(filtered_dataset["rejected"])
            == len(filtered_dataset["chosen_chat_score"])
        )
        dpo_dataset = Dataset.from_dict(filtered_dataset)
        # Drop the scores since they are not needed for training
        dpo_dataset = dpo_dataset.remove_columns("chosen_chat_score")
        return dpo_dataset

    def get_highest_score(self):
        return max(self.train_dataset["chosen_chat_score"])

    def __len__(self):
        return len(self.train_dataset["prompt"])


